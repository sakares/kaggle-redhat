{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network\n",
    "Simple NN implementation for redhat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class `TwoLayerNet` in the file `cs231n/classifiers/neural_net.py` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load complete\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('act_train.csv')\n",
    "people_df = pd.read_csv('people.csv')\n",
    "merged_df = pd.merge(train_df, people_df, on='people_id')\n",
    "test_df = pd.read_csv('act_test.csv')\n",
    "\n",
    "merged_col = merged_df.columns\n",
    "summary_merged_df = [len(merged_df[col].unique()) for col in merged_col]\n",
    "print \"load complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col = test_df.columns\n",
    "summary_test_df = [len(test_df[col].unique()) for col in test_col]\n",
    "\n",
    "setA = set(train_df['char_8'])\n",
    "setB = set(test_df['char_8'])\n",
    "setA.union(setB) - setA.intersection(setB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert attributes\n",
    "- Categorical : type ## / NaN\n",
    "- Boolean (True False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1\n",
      "- is type/category\n",
      "char_12\n",
      "- is boolean\n",
      "char_33\n",
      "- is boolean\n",
      "char_2_y\n",
      "- is type/category\n",
      "activity_category\n",
      "- is type/category\n",
      "char_38\n",
      "- is integer\n",
      "char_1_y\n",
      "- is type/category\n",
      "char_8_y\n",
      "- is type/category\n",
      "char_19\n",
      "- is boolean\n",
      "char_18\n",
      "- is boolean\n",
      "char_17\n",
      "- is boolean\n",
      "char_8_x\n",
      "- is type/category\n",
      "char_15\n",
      "- is boolean\n",
      "char_32\n",
      "- is boolean\n",
      "char_9_y\n",
      "- is type/category\n",
      "char_9_x\n",
      "- is type/category\n",
      "char_11\n",
      "- is boolean\n",
      "char_1_x\n",
      "- is type/category\n",
      "char_3_y\n",
      "- is type/category\n",
      "char_3_x\n",
      "- is type/category\n",
      "char_6_y\n",
      "- is type/category\n",
      "char_5_y\n",
      "- is type/category\n",
      "char_31\n",
      "- is boolean\n",
      "outcome\n",
      "- is integer\n",
      "char_25\n",
      "- is boolean\n",
      "char_10_x\n",
      "- is type/category\n",
      "char_10_y\n",
      "- is boolean\n",
      "char_30\n",
      "- is boolean\n",
      "char_2_x\n",
      "- is type/category\n",
      "char_35\n",
      "- is boolean\n",
      "char_16\n",
      "- is boolean\n",
      "char_26\n",
      "- is boolean\n",
      "char_27\n",
      "- is boolean\n",
      "char_24\n",
      "- is boolean\n",
      "char_14\n",
      "- is boolean\n",
      "char_22\n",
      "- is boolean\n",
      "char_23\n",
      "- is boolean\n",
      "char_20\n",
      "- is boolean\n",
      "char_5_x\n",
      "- is type/category\n",
      "char_4_x\n",
      "- is type/category\n",
      "char_13\n",
      "- is boolean\n",
      "char_21\n",
      "- is boolean\n",
      "char_28\n",
      "- is boolean\n",
      "char_29\n",
      "- is boolean\n",
      "char_37\n",
      "- is boolean\n",
      "char_6_x\n",
      "- is type/category\n",
      "char_4_y\n",
      "- is type/category\n",
      "char_34\n",
      "- is boolean\n",
      "char_36\n",
      "- is boolean\n",
      "char_7_y\n",
      "- is type/category\n",
      "char_7_x\n",
      "- is type/category\n"
     ]
    }
   ],
   "source": [
    "#merged_df['char_1_x'].replace({'type':'', ' ':'', np.nan : '0'}, regex = True).astype('int')\n",
    "#Columns to be converted\n",
    "merged_df = pd.merge(train_df, people_df, on='people_id')\n",
    "parsed_col = set(merged_col) - set(['people_id','activity_id','date_x','date_y'])\n",
    "for col in parsed_col:\n",
    "    print (col)\n",
    "    if(type(merged_df[col][0]) is np.bool_):\n",
    "        print('- is boolean')\n",
    "        merged_df[col] = merged_df[col].astype('int64')\n",
    "    elif (type(merged_df[col][0]) is np.int64 or type(merged_df[col][0]) is np.long):\n",
    "        print('- is integer')\n",
    "        merged_df[col] = merged_df[col].astype('int64')\n",
    "    else:\n",
    "        print('- is type/category')\n",
    "        merged_df[col] = merged_df[col].replace({'group': '', 'type':'', ' ':'', np.nan : '0'}, regex = True).astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1\n",
      "- is type/category\n",
      "char_12\n",
      "- is boolean\n",
      "char_33\n",
      "- is boolean\n",
      "char_2_y\n",
      "- is type/category\n",
      "activity_category\n",
      "- is type/category\n",
      "char_38\n",
      "- is integer\n",
      "char_1_y\n",
      "- is type/category\n",
      "char_8_y\n",
      "- is type/category\n",
      "char_19\n",
      "- is boolean\n",
      "char_18\n",
      "- is boolean\n",
      "char_17\n",
      "- is boolean\n",
      "char_8_x\n",
      "- is type/category\n",
      "char_15\n",
      "- is boolean\n",
      "char_32\n",
      "- is boolean\n",
      "char_9_y\n",
      "- is type/category\n",
      "char_9_x\n",
      "- is type/category\n",
      "char_11\n",
      "- is boolean\n",
      "char_1_x\n",
      "- is type/category\n",
      "char_3_y\n",
      "- is type/category\n",
      "char_3_x\n",
      "- is type/category\n",
      "char_6_y\n",
      "- is type/category\n",
      "char_5_y\n",
      "- is type/category\n",
      "char_31\n",
      "- is boolean\n",
      "char_25\n",
      "- is boolean\n",
      "char_10_x\n",
      "- is type/category\n",
      "char_10_y\n",
      "- is boolean\n",
      "char_30\n",
      "- is boolean\n",
      "char_2_x\n",
      "- is type/category\n",
      "char_35\n",
      "- is boolean\n",
      "char_16\n",
      "- is boolean\n",
      "char_26\n",
      "- is boolean\n",
      "char_27\n",
      "- is boolean\n",
      "char_24\n",
      "- is boolean\n",
      "char_14\n",
      "- is boolean\n",
      "char_22\n",
      "- is boolean\n",
      "char_23\n",
      "- is boolean\n",
      "char_20\n",
      "- is boolean\n",
      "char_5_x\n",
      "- is type/category\n",
      "char_4_x\n",
      "- is type/category\n",
      "char_13\n",
      "- is boolean\n",
      "char_21\n",
      "- is boolean\n",
      "char_28\n",
      "- is boolean\n",
      "char_29\n",
      "- is boolean\n",
      "char_37\n",
      "- is boolean\n",
      "char_6_x\n",
      "- is type/category\n",
      "char_4_y\n",
      "- is type/category\n",
      "char_34\n",
      "- is boolean\n",
      "char_36\n",
      "- is boolean\n",
      "char_7_y\n",
      "- is type/category\n",
      "char_7_x\n",
      "- is type/category\n"
     ]
    }
   ],
   "source": [
    "merged_test_df = pd.merge(test_df, people_df, on='people_id')\n",
    "merged_test_col = merged_test_df.columns\n",
    "parsed_test_col = set(merged_test_col) - set(['people_id','activity_id','date_x','date_y'])\n",
    "for col in parsed_test_col:\n",
    "    print (col)\n",
    "    if(type(merged_test_df[col][0]) is np.bool_):\n",
    "        print('- is boolean')\n",
    "        merged_test_df[col] = merged_test_df[col].astype('int64')\n",
    "    elif (type(merged_test_df[col][0]) is np.int64 or type(merged_test_df[col][0]) is np.long):\n",
    "        print('- is integer')\n",
    "        merged_test_df[col] = merged_test_df[col].astype('int64')\n",
    "    else:\n",
    "        print('- is type/category')\n",
    "        merged_test_df[col] = merged_test_df[col].replace({'group': '', 'type':'', ' ':'', np.nan : '0'}, regex = True).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   char_33  char_34  char_35  char_36  char_37\n",
      "0        0        1        1        1        0\n",
      "1        0        1        1        1        0\n",
      "2        0        1        1        1        0\n",
      "3        0        1        1        1        0\n",
      "4        0        1        1        1        0\n",
      "5        0        1        1        1        0\n",
      "6        1        1        1        1        0\n",
      "7        1        1        1        1        0\n",
      "8        1        1        0        1        1\n",
      "9        1        1        0        1        1\n",
      "[[0 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 0 1 1]\n",
      " [1 1 0 1 1]]\n",
      "[0 0 0 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# y = merged_df['outcome'].values\n",
    "# tmp = merged_df.drop(['people_id','activity_id','date_x','date_y','outcome'], 1)\n",
    "# tmp = tmp[['char_33','char_34','char_35','char_36','char_37']]\n",
    "# print tmp.head(10)\n",
    "# X = tmp.values[0:10,:]\n",
    "# print X\n",
    "# print y\n",
    "# tmp.head(10)\n",
    "# y.head(10)\n",
    "\n",
    "# print 'length of ppl_100 = %d ' % len(check_df)\n",
    "# print check_df['outcome']\n",
    "# print check_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = merged_df.drop(['people_id','activity_id','date_x','date_y','outcome'], 1)\n",
    "# train_outcome = np.array(merged_df.outcome)\n",
    "# train_mat = np.array(train_dataset, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_outcome.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 16.24345364  -6.11756414  -5.28171752 -10.72968622   8.65407629]\n",
      " [-23.01538697  17.44811764  -7.61206901   3.19039096  -2.49370375]\n",
      " [ 14.62107937 -20.60140709  -3.22417204  -3.84054355  11.33769442]\n",
      " [-10.99891267  -1.72428208  -8.77858418   0.42213747   5.82815214]\n",
      " [-11.00619177  11.4472371    9.01590721   5.02494339   9.00855949]\n",
      " [ -6.83727859  -1.22890226  -9.35769434  -2.6788808    5.30355467]\n",
      " [ -6.91660752  -3.96753527  -6.871727    -8.45205641  -6.71246131]\n",
      " [ -0.12664599 -11.17310349   2.34415698  16.59802177   7.42044161]\n",
      " [ -1.91835552  -8.87628964  -7.47158294  16.92454601   0.50807755]\n",
      " [ -6.36995647   1.90915485  21.00255136   1.20158952   6.1720311 ]]\n",
      "[0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "# input_size = 4\n",
    "# hidden_size = 10\n",
    "# num_classes = 3\n",
    "# num_inputs = 5\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "num_inputs = 10\n",
    "\n",
    "def init_toy_model():\n",
    "  np.random.seed(0)\n",
    "  return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "  np.random.seed(1)\n",
    "  X = 10 * np.random.randn(num_inputs, input_size)\n",
    "  y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "  return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "print X\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `cs231n/classifiers/neural_net.py` and look at the method `TwoLayerNet.loss`. This function is very similar to the loss functions you have written for the SVM and Softmax exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. \n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.36710471  0.30791951]\n",
      " [-0.43989025 -0.53666637]\n",
      " [-0.31311846  0.21907549]\n",
      " [-0.12210622 -0.10849958]\n",
      " [-0.06744353 -0.15266295]\n",
      " [-0.15434499 -0.03394499]\n",
      " [-0.25922861  0.00724488]\n",
      " [-0.00854613 -0.10759501]\n",
      " [-0.22549414 -0.15963828]\n",
      " [-0.83546277 -0.07105516]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.36681027  0.30681027]\n",
      " [-0.43681027 -0.53681027]\n",
      " [-0.31681027  0.21681027]\n",
      " [-0.12681027 -0.10681027]\n",
      " [-0.06681027 -0.15681027]\n",
      " [-0.15681027 -0.03681027]\n",
      " [-0.25681027  0.00681027]\n",
      " [-0.00681027 -0.10681027]\n",
      " [-0.22681027 -0.15681027]\n",
      " [-0.83681027 -0.07681027]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "0.0437093888987\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print 'Your scores:'\n",
    "print scores\n",
    "print\n",
    "print 'correct scores:'\n",
    "correct_scores = np.asarray([\n",
    " [-0.36681027,  0.30681027],\n",
    " [-0.43681027, -0.53681027],\n",
    " [-0.31681027,  0.21681027],\n",
    " [-0.12681027, -0.10681027],\n",
    " [-0.06681027, -0.15681027],\n",
    " [-0.15681027, -0.03681027],\n",
    " [-0.25681027,  0.00681027],\n",
    " [-0.00681027, -0.10681027],\n",
    " [-0.22681027, -0.15681027],\n",
    " [-0.83681027, -0.07681027]])\n",
    "print correct_scores\n",
    "print\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print 'Difference between your scores and correct scores:'\n",
    "print np.sum(np.abs(scores - correct_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "0.517447048212\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.30378789133 # dummy value\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print 'Difference between your loss and correct loss:'\n",
    "print np.sum(np.abs(loss - correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2 max relative error: 1.039752e-10\n",
      "W2 max relative error: 2.312778e-10\n",
      "W1 max relative error: 1.367135e-09\n",
      "b1 max relative error: 1.599280e-09\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "  f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "  param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "  print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function `TwoLayerNet.train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement `TwoLayerNet.predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.132510738892\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAH4CAYAAAD+YRGXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElSxxigSZIkdczfaocTK3gx4mMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112f2ad10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "# print X\n",
    "# print y\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=1e-5,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print 'Final training loss: ', stats['loss_history'][-1]\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197291, 50)\n",
      "Train data shape:  (1406266, 50)\n",
      "Train labels shape:  (1406266,)\n",
      "Validation data shape:  (351567, 50)\n",
      "Validation labels shape:  (351567,)\n",
      "Test data shape:  (439458, 50)\n",
      "Test labels shape:  (439458,)\n"
     ]
    }
   ],
   "source": [
    "y = merged_df['outcome'].values\n",
    "tmp = merged_df.drop(['people_id','activity_id','date_x','date_y','outcome'], 1)\n",
    "X = tmp.values\n",
    "print X.shape\n",
    "#print X[3,:]\n",
    "# print tmp.head(10)\n",
    "X_train = X[0:1406266,:]\n",
    "y_train = y[0:1406266]\n",
    "X_val = X[1406266:1757833,:]\n",
    "y_val = y[1406266:1757833]\n",
    "X_test = X[1757833:2197291,:]\n",
    "y_test = y[1757833:2197291]\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(490,10,10)\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "To train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 40000: loss 0.693173\n",
      "iteration 100 / 40000: loss 0.677820\n",
      "iteration 200 / 40000: loss 0.669449\n",
      "iteration 300 / 40000: loss 0.656030\n",
      "iteration 400 / 40000: loss 0.675461\n",
      "iteration 500 / 40000: loss 0.680426\n",
      "iteration 600 / 40000: loss 0.696262\n",
      "iteration 700 / 40000: loss 0.672914\n",
      "iteration 800 / 40000: loss 0.650694\n",
      "iteration 900 / 40000: loss 0.680168\n",
      "iteration 1000 / 40000: loss 0.691477\n",
      "iteration 1100 / 40000: loss 0.676305\n",
      "iteration 1200 / 40000: loss 0.666728\n",
      "iteration 1300 / 40000: loss 0.691261\n",
      "iteration 1400 / 40000: loss 0.668496\n",
      "iteration 1500 / 40000: loss 0.654361\n",
      "iteration 1600 / 40000: loss 0.667894\n",
      "iteration 1700 / 40000: loss 0.672326\n",
      "iteration 1800 / 40000: loss 0.692539\n",
      "iteration 1900 / 40000: loss 0.664164\n",
      "iteration 2000 / 40000: loss 0.659943\n",
      "iteration 2100 / 40000: loss 0.676064\n",
      "iteration 2200 / 40000: loss 0.675648\n",
      "iteration 2300 / 40000: loss 0.679032\n",
      "iteration 2400 / 40000: loss 0.659287\n",
      "iteration 2500 / 40000: loss 0.674231\n",
      "iteration 2600 / 40000: loss 0.699872\n",
      "iteration 2700 / 40000: loss 0.682516\n",
      "iteration 2800 / 40000: loss 0.678787\n",
      "iteration 2900 / 40000: loss 0.660697\n",
      "iteration 3000 / 40000: loss 0.647759\n",
      "iteration 3100 / 40000: loss 0.668674\n",
      "iteration 3200 / 40000: loss 0.666773\n",
      "iteration 3300 / 40000: loss 0.666267\n",
      "iteration 3400 / 40000: loss 0.659791\n",
      "iteration 3500 / 40000: loss 0.623717\n",
      "iteration 3600 / 40000: loss 0.671580\n",
      "iteration 3700 / 40000: loss 0.680557\n",
      "iteration 3800 / 40000: loss 0.676709\n",
      "iteration 3900 / 40000: loss 0.676283\n",
      "iteration 4000 / 40000: loss 0.657058\n",
      "iteration 4100 / 40000: loss 0.645426\n",
      "iteration 4200 / 40000: loss 0.659281\n",
      "iteration 4300 / 40000: loss 0.709480\n",
      "iteration 4400 / 40000: loss 0.697715\n",
      "iteration 4500 / 40000: loss 0.669932\n",
      "iteration 4600 / 40000: loss 0.669785\n",
      "iteration 4700 / 40000: loss 0.665570\n",
      "iteration 4800 / 40000: loss 0.674537\n",
      "iteration 4900 / 40000: loss 0.680628\n",
      "iteration 5000 / 40000: loss 0.707501\n",
      "iteration 5100 / 40000: loss 0.662913\n",
      "iteration 5200 / 40000: loss 0.687386\n",
      "iteration 5300 / 40000: loss 0.687899\n",
      "iteration 5400 / 40000: loss 0.656838\n",
      "iteration 5500 / 40000: loss 0.667157\n",
      "iteration 5600 / 40000: loss 0.685726\n",
      "iteration 5700 / 40000: loss 0.643822\n",
      "iteration 5800 / 40000: loss 0.611268\n",
      "iteration 5900 / 40000: loss 0.647143\n",
      "iteration 6000 / 40000: loss 0.646940\n",
      "iteration 6100 / 40000: loss 0.600969\n",
      "iteration 6200 / 40000: loss 0.647337\n",
      "iteration 6300 / 40000: loss 0.685762\n",
      "iteration 6400 / 40000: loss 0.669002\n",
      "iteration 6500 / 40000: loss 0.658383\n",
      "iteration 6600 / 40000: loss 0.644784\n",
      "iteration 6700 / 40000: loss 0.657014\n",
      "iteration 6800 / 40000: loss 0.634516\n",
      "iteration 6900 / 40000: loss 0.700406\n",
      "iteration 7000 / 40000: loss 0.680200\n",
      "iteration 7100 / 40000: loss 0.656415\n",
      "iteration 7200 / 40000: loss 0.635438\n",
      "iteration 7300 / 40000: loss 0.671256\n",
      "iteration 7400 / 40000: loss 0.627112\n",
      "iteration 7500 / 40000: loss 0.674134\n",
      "iteration 7600 / 40000: loss 0.625420\n",
      "iteration 7700 / 40000: loss 0.652276\n",
      "iteration 7800 / 40000: loss 0.626235\n",
      "iteration 7900 / 40000: loss 0.634353\n",
      "iteration 8000 / 40000: loss 0.646712\n",
      "iteration 8100 / 40000: loss 0.667702\n",
      "iteration 8200 / 40000: loss 0.675487\n",
      "iteration 8300 / 40000: loss 0.681268\n",
      "iteration 8400 / 40000: loss 0.674688\n",
      "iteration 8500 / 40000: loss 0.642789\n",
      "iteration 8600 / 40000: loss 0.675125\n",
      "iteration 8700 / 40000: loss 0.652037\n",
      "iteration 8800 / 40000: loss 0.655023\n",
      "iteration 8900 / 40000: loss 0.668705\n",
      "iteration 9000 / 40000: loss 0.674932\n",
      "iteration 9100 / 40000: loss 0.664827\n",
      "iteration 9200 / 40000: loss 0.682398\n",
      "iteration 9300 / 40000: loss 0.693644\n",
      "iteration 9400 / 40000: loss 0.649909\n",
      "iteration 9500 / 40000: loss 0.638707\n",
      "iteration 9600 / 40000: loss 0.626619\n",
      "iteration 9700 / 40000: loss 0.666745\n",
      "iteration 9800 / 40000: loss 0.691389\n",
      "iteration 9900 / 40000: loss 0.639571\n",
      "iteration 10000 / 40000: loss 0.646620\n",
      "iteration 10100 / 40000: loss 0.669825\n",
      "iteration 10200 / 40000: loss 0.670595\n",
      "iteration 10300 / 40000: loss 0.685414\n",
      "iteration 10400 / 40000: loss 0.604915\n",
      "iteration 10500 / 40000: loss 0.670684\n",
      "iteration 10600 / 40000: loss 0.649166\n",
      "iteration 10700 / 40000: loss 0.630903\n",
      "iteration 10800 / 40000: loss 0.629593\n",
      "iteration 10900 / 40000: loss 0.691061\n",
      "iteration 11000 / 40000: loss 0.654668\n",
      "iteration 11100 / 40000: loss 0.670129\n",
      "iteration 11200 / 40000: loss 0.685176\n",
      "iteration 11300 / 40000: loss 0.658852\n",
      "iteration 11400 / 40000: loss 0.647551\n",
      "iteration 11500 / 40000: loss 0.695019\n",
      "iteration 11600 / 40000: loss 0.654847\n",
      "iteration 11700 / 40000: loss 0.655192\n",
      "iteration 11800 / 40000: loss 0.653324\n",
      "iteration 11900 / 40000: loss 0.629785\n",
      "iteration 12000 / 40000: loss 0.684804\n",
      "iteration 12100 / 40000: loss 0.679358\n",
      "iteration 12200 / 40000: loss 0.639106\n",
      "iteration 12300 / 40000: loss 0.605227\n",
      "iteration 12400 / 40000: loss 0.655119\n",
      "iteration 12500 / 40000: loss 0.660931\n",
      "iteration 12600 / 40000: loss 0.661143\n",
      "iteration 12700 / 40000: loss 0.621775\n",
      "iteration 12800 / 40000: loss 0.753213\n",
      "iteration 12900 / 40000: loss 0.620300\n",
      "iteration 13000 / 40000: loss 0.653766\n",
      "iteration 13100 / 40000: loss 0.625285\n",
      "iteration 13200 / 40000: loss 0.691072\n",
      "iteration 13300 / 40000: loss 0.638343\n",
      "iteration 13400 / 40000: loss 0.656500\n",
      "iteration 13500 / 40000: loss 0.681238\n",
      "iteration 13600 / 40000: loss 0.745899\n",
      "iteration 13700 / 40000: loss 0.656202\n",
      "iteration 13800 / 40000: loss 0.642009\n",
      "iteration 13900 / 40000: loss 0.613980\n",
      "iteration 14000 / 40000: loss 0.682041\n",
      "iteration 14100 / 40000: loss 0.649879\n",
      "iteration 14200 / 40000: loss 0.682368\n",
      "iteration 14300 / 40000: loss 0.641222\n",
      "iteration 14400 / 40000: loss 0.650364\n",
      "iteration 14500 / 40000: loss 0.641164\n",
      "iteration 14600 / 40000: loss 0.684962\n",
      "iteration 14700 / 40000: loss 0.647793\n",
      "iteration 14800 / 40000: loss 0.631632\n",
      "iteration 14900 / 40000: loss 0.663341\n",
      "iteration 15000 / 40000: loss 0.637923\n",
      "iteration 15100 / 40000: loss 0.695410\n",
      "iteration 15200 / 40000: loss 0.691726\n",
      "iteration 15300 / 40000: loss 0.689282\n",
      "iteration 15400 / 40000: loss 0.698316\n",
      "iteration 15500 / 40000: loss 0.689649\n",
      "iteration 15600 / 40000: loss 0.647011\n",
      "iteration 15700 / 40000: loss 0.650029\n",
      "iteration 15800 / 40000: loss 0.656707\n",
      "iteration 15900 / 40000: loss 0.668546\n",
      "iteration 16000 / 40000: loss 0.660917\n",
      "iteration 16100 / 40000: loss 0.655844\n",
      "iteration 16200 / 40000: loss 0.614939\n",
      "iteration 16300 / 40000: loss 0.637489\n",
      "iteration 16400 / 40000: loss 0.640658\n",
      "iteration 16500 / 40000: loss 0.652621\n",
      "iteration 16600 / 40000: loss 0.711563\n",
      "iteration 16700 / 40000: loss 0.653505\n",
      "iteration 16800 / 40000: loss 0.688619\n",
      "iteration 16900 / 40000: loss 0.695223\n",
      "iteration 17000 / 40000: loss 0.668416\n",
      "iteration 17100 / 40000: loss 0.684632\n",
      "iteration 17200 / 40000: loss 0.641325\n",
      "iteration 17300 / 40000: loss 0.648383\n",
      "iteration 17400 / 40000: loss 0.660404\n",
      "iteration 17500 / 40000: loss 0.682413\n",
      "iteration 17600 / 40000: loss 0.726983\n",
      "iteration 17700 / 40000: loss 0.688245\n",
      "iteration 17800 / 40000: loss 0.657161\n",
      "iteration 17900 / 40000: loss 0.634389\n",
      "iteration 18000 / 40000: loss 0.651716\n",
      "iteration 18100 / 40000: loss 0.691368\n",
      "iteration 18200 / 40000: loss 0.636450\n",
      "iteration 18300 / 40000: loss 0.686371\n",
      "iteration 18400 / 40000: loss 0.638598\n",
      "iteration 18500 / 40000: loss 0.631017\n",
      "iteration 18600 / 40000: loss 0.659808\n",
      "iteration 18700 / 40000: loss 0.631195\n",
      "iteration 18800 / 40000: loss 0.660678\n",
      "iteration 18900 / 40000: loss 0.644841\n",
      "iteration 19000 / 40000: loss 0.666077\n",
      "iteration 19100 / 40000: loss 0.642216\n",
      "iteration 19200 / 40000: loss 0.609680\n",
      "iteration 19300 / 40000: loss 0.650223\n",
      "iteration 19400 / 40000: loss 0.650968\n",
      "iteration 19500 / 40000: loss 0.703081\n",
      "iteration 19600 / 40000: loss 0.645106\n",
      "iteration 19700 / 40000: loss 0.606933\n",
      "iteration 19800 / 40000: loss 0.681141\n",
      "iteration 19900 / 40000: loss 0.650166\n",
      "iteration 20000 / 40000: loss 0.675668\n",
      "iteration 20100 / 40000: loss 0.680332\n",
      "iteration 20200 / 40000: loss 0.636144\n",
      "iteration 20300 / 40000: loss 0.645377\n",
      "iteration 20400 / 40000: loss 0.669398\n",
      "iteration 20500 / 40000: loss 0.639426\n",
      "iteration 20600 / 40000: loss 0.663149\n",
      "iteration 20700 / 40000: loss 0.619107\n",
      "iteration 20800 / 40000: loss 0.628769\n",
      "iteration 20900 / 40000: loss 0.654731\n",
      "iteration 21000 / 40000: loss 0.677534\n",
      "iteration 21100 / 40000: loss 0.653739\n",
      "iteration 21200 / 40000: loss 0.661748\n",
      "iteration 21300 / 40000: loss 0.595003\n",
      "iteration 21400 / 40000: loss 0.629723\n",
      "iteration 21500 / 40000: loss 0.629844\n",
      "iteration 21600 / 40000: loss 0.638032\n",
      "iteration 21700 / 40000: loss 0.631512\n",
      "iteration 21800 / 40000: loss 0.618805\n",
      "iteration 21900 / 40000: loss 0.649654\n",
      "iteration 22000 / 40000: loss 0.697567\n",
      "iteration 22100 / 40000: loss 0.658112\n",
      "iteration 22200 / 40000: loss 0.663991\n",
      "iteration 22300 / 40000: loss 0.631064\n",
      "iteration 22400 / 40000: loss 0.641258\n",
      "iteration 22500 / 40000: loss 0.679723\n",
      "iteration 22600 / 40000: loss 0.620537\n",
      "iteration 22700 / 40000: loss 0.645865\n",
      "iteration 22800 / 40000: loss 0.642508\n",
      "iteration 22900 / 40000: loss 0.634788\n",
      "iteration 23000 / 40000: loss 0.641102\n",
      "iteration 23100 / 40000: loss 0.581051\n",
      "iteration 23200 / 40000: loss 0.651992\n",
      "iteration 23300 / 40000: loss 0.628623\n",
      "iteration 23400 / 40000: loss 0.648526\n",
      "iteration 23500 / 40000: loss 0.595831\n",
      "iteration 23600 / 40000: loss 0.673896\n",
      "iteration 23700 / 40000: loss 0.620760\n",
      "iteration 23800 / 40000: loss 0.642451\n",
      "iteration 23900 / 40000: loss 0.648432\n",
      "iteration 24000 / 40000: loss 0.602380\n",
      "iteration 24100 / 40000: loss 0.609515\n",
      "iteration 24200 / 40000: loss 0.611826\n",
      "iteration 24300 / 40000: loss 0.622920\n",
      "iteration 24400 / 40000: loss 0.660753\n",
      "iteration 24500 / 40000: loss 0.642035\n",
      "iteration 24600 / 40000: loss 0.622315\n",
      "iteration 24700 / 40000: loss 0.634138\n",
      "iteration 24800 / 40000: loss 0.640394\n",
      "iteration 24900 / 40000: loss 0.637278\n",
      "iteration 25000 / 40000: loss 0.685366\n",
      "iteration 25100 / 40000: loss 0.655764\n",
      "iteration 25200 / 40000: loss 0.655992\n",
      "iteration 25300 / 40000: loss 0.743998\n",
      "iteration 25400 / 40000: loss 0.641551\n",
      "iteration 25500 / 40000: loss 0.602194\n",
      "iteration 25600 / 40000: loss 0.674571\n",
      "iteration 25700 / 40000: loss 0.621482\n",
      "iteration 25800 / 40000: loss 0.699417\n",
      "iteration 25900 / 40000: loss 0.629990\n",
      "iteration 26000 / 40000: loss 0.642088\n",
      "iteration 26100 / 40000: loss 0.666003\n",
      "iteration 26200 / 40000: loss 0.649804\n",
      "iteration 26300 / 40000: loss 0.692019\n",
      "iteration 26400 / 40000: loss 0.627972\n",
      "iteration 26500 / 40000: loss 0.700003\n",
      "iteration 26600 / 40000: loss 0.650010\n",
      "iteration 26700 / 40000: loss 0.643874\n",
      "iteration 26800 / 40000: loss 0.645821\n",
      "iteration 26900 / 40000: loss 0.652686\n",
      "iteration 27000 / 40000: loss 0.642439\n",
      "iteration 27100 / 40000: loss 0.645806\n",
      "iteration 27200 / 40000: loss 0.636391\n",
      "iteration 27300 / 40000: loss 0.691509\n",
      "iteration 27400 / 40000: loss 0.620226\n",
      "iteration 27500 / 40000: loss 0.658680\n",
      "iteration 27600 / 40000: loss 0.638602\n",
      "iteration 27700 / 40000: loss 0.646368\n",
      "iteration 27800 / 40000: loss 0.603399\n",
      "iteration 27900 / 40000: loss 0.727355\n",
      "iteration 28000 / 40000: loss 0.580877\n",
      "iteration 28100 / 40000: loss 0.685571\n",
      "iteration 28200 / 40000: loss 0.636548\n",
      "iteration 28300 / 40000: loss 0.664857\n",
      "iteration 28400 / 40000: loss 0.632895\n",
      "iteration 28500 / 40000: loss 0.599093\n",
      "iteration 28600 / 40000: loss 0.623998\n",
      "iteration 28700 / 40000: loss 0.649617\n",
      "iteration 28800 / 40000: loss 0.682264\n",
      "iteration 28900 / 40000: loss 0.706215\n",
      "iteration 29000 / 40000: loss 0.672436\n",
      "iteration 29100 / 40000: loss 0.621296\n",
      "iteration 29200 / 40000: loss 0.592317\n",
      "iteration 29300 / 40000: loss 0.639056\n",
      "iteration 29400 / 40000: loss 0.663035\n",
      "iteration 29500 / 40000: loss 0.618888\n",
      "iteration 29600 / 40000: loss 0.629585\n",
      "iteration 29700 / 40000: loss 0.589007\n",
      "iteration 29800 / 40000: loss 0.655592\n",
      "iteration 29900 / 40000: loss 0.708483\n",
      "iteration 30000 / 40000: loss 0.629493\n",
      "iteration 30100 / 40000: loss 0.592281\n",
      "iteration 30200 / 40000: loss 0.618198\n",
      "iteration 30300 / 40000: loss 0.738191\n",
      "iteration 30400 / 40000: loss 0.688496\n",
      "iteration 30500 / 40000: loss 0.631129\n",
      "iteration 30600 / 40000: loss 0.673996\n",
      "iteration 30700 / 40000: loss 0.628135\n",
      "iteration 30800 / 40000: loss 0.646912\n",
      "iteration 30900 / 40000: loss 0.599648\n",
      "iteration 31000 / 40000: loss 0.637238\n",
      "iteration 31100 / 40000: loss 0.656045\n",
      "iteration 31200 / 40000: loss 0.642237\n",
      "iteration 31300 / 40000: loss 0.646628\n",
      "iteration 31400 / 40000: loss 0.641406\n",
      "iteration 31500 / 40000: loss 0.585854\n",
      "iteration 31600 / 40000: loss 0.646284\n",
      "iteration 31700 / 40000: loss 0.663571\n",
      "iteration 31800 / 40000: loss 0.606204\n",
      "iteration 31900 / 40000: loss 0.618567\n",
      "iteration 32000 / 40000: loss 0.601196\n",
      "iteration 32100 / 40000: loss 0.617131\n",
      "iteration 32200 / 40000: loss 0.590555\n",
      "iteration 32300 / 40000: loss 0.590553\n",
      "iteration 32400 / 40000: loss 0.671018\n",
      "iteration 32500 / 40000: loss 0.673645\n",
      "iteration 32600 / 40000: loss 0.657267\n",
      "iteration 32700 / 40000: loss 0.664300\n",
      "iteration 32800 / 40000: loss 0.613278\n",
      "iteration 32900 / 40000: loss 0.694824\n",
      "iteration 33000 / 40000: loss 0.654082\n",
      "iteration 33100 / 40000: loss 0.638539\n",
      "iteration 33200 / 40000: loss 0.588367\n",
      "iteration 33300 / 40000: loss 0.666735\n",
      "iteration 33400 / 40000: loss 0.634246\n",
      "iteration 33500 / 40000: loss 0.604191\n",
      "iteration 33600 / 40000: loss 0.629684\n",
      "iteration 33700 / 40000: loss 0.638898\n",
      "iteration 33800 / 40000: loss 0.666556\n",
      "iteration 33900 / 40000: loss 0.600364\n",
      "iteration 34000 / 40000: loss 0.638695\n",
      "iteration 34100 / 40000: loss 0.625024\n",
      "iteration 34200 / 40000: loss 0.584418\n",
      "iteration 34300 / 40000: loss 0.647691\n",
      "iteration 34400 / 40000: loss 0.609334\n",
      "iteration 34500 / 40000: loss 0.638013\n",
      "iteration 34600 / 40000: loss 0.624017\n",
      "iteration 34700 / 40000: loss 0.698626\n",
      "iteration 34800 / 40000: loss 0.640872\n",
      "iteration 34900 / 40000: loss 0.616147\n",
      "iteration 35000 / 40000: loss 0.699882\n",
      "iteration 35100 / 40000: loss 0.643539\n",
      "iteration 35200 / 40000: loss 0.587838\n",
      "iteration 35300 / 40000: loss 0.602247\n",
      "iteration 35400 / 40000: loss 0.611469\n",
      "iteration 35500 / 40000: loss 0.619285\n",
      "iteration 35600 / 40000: loss 0.596459\n",
      "iteration 35700 / 40000: loss 0.649937\n",
      "iteration 35800 / 40000: loss 1.065897\n",
      "iteration 35900 / 40000: loss 0.681709\n",
      "iteration 36000 / 40000: loss 0.617020\n",
      "iteration 36100 / 40000: loss 0.625618\n",
      "iteration 36200 / 40000: loss 0.671071\n",
      "iteration 36300 / 40000: loss 0.697375\n",
      "iteration 36400 / 40000: loss 0.632220\n",
      "iteration 36500 / 40000: loss 0.666156\n",
      "iteration 36600 / 40000: loss 0.593023\n",
      "iteration 36700 / 40000: loss 0.594844\n",
      "iteration 36800 / 40000: loss 0.595662\n",
      "iteration 36900 / 40000: loss 0.648457\n",
      "iteration 37000 / 40000: loss 0.660491\n",
      "iteration 37100 / 40000: loss 0.649683\n",
      "iteration 37200 / 40000: loss 0.602330\n",
      "iteration 37300 / 40000: loss 0.607820\n",
      "iteration 37400 / 40000: loss 0.606254\n",
      "iteration 37500 / 40000: loss 0.597413\n",
      "iteration 37600 / 40000: loss 0.671183\n",
      "iteration 37700 / 40000: loss 0.676394\n",
      "iteration 37800 / 40000: loss 0.611879\n",
      "iteration 37900 / 40000: loss 0.621484\n",
      "iteration 38000 / 40000: loss 0.635384\n",
      "iteration 38100 / 40000: loss 0.676266\n",
      "iteration 38200 / 40000: loss 0.619205\n",
      "iteration 38300 / 40000: loss 0.565114\n",
      "iteration 38400 / 40000: loss 0.628604\n",
      "iteration 38500 / 40000: loss 0.603075\n",
      "iteration 38600 / 40000: loss 0.622065\n",
      "iteration 38700 / 40000: loss 0.608174\n",
      "iteration 38800 / 40000: loss 0.653977\n",
      "iteration 38900 / 40000: loss 0.621774\n",
      "iteration 39000 / 40000: loss 0.591254\n",
      "iteration 39100 / 40000: loss 0.566490\n",
      "iteration 39200 / 40000: loss 0.586358\n",
      "iteration 39300 / 40000: loss 0.664307\n",
      "iteration 39400 / 40000: loss 0.655970\n",
      "iteration 39500 / 40000: loss 0.625079\n",
      "iteration 39600 / 40000: loss 0.628025\n",
      "iteration 39700 / 40000: loss 0.619814\n",
      "iteration 39800 / 40000: loss 0.627703\n",
      "iteration 39900 / 40000: loss 0.614284\n",
      "Validation accuracy:  0.753870528235\n"
     ]
    }
   ],
   "source": [
    "# input_size = 32 * 32 * 3\n",
    "# hidden_size = 50\n",
    "# num_classes = 10\n",
    "\n",
    "input_size = 50\n",
    "hidden_size = 30\n",
    "num_classes = 2\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=40000, batch_size=200,\n",
    "            learning_rate=0.000122574676049, learning_rate_decay=0.95,\n",
    "            reg=0.000257796641079, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print 'Validation accuracy: ', val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498687, 50)\n"
     ]
    }
   ],
   "source": [
    "activity_id = list(merged_test_df.activity_id)\n",
    "test_dataset = merged_test_df.drop(['people_id','activity_id','date_x','date_y'], 1)\n",
    "test_mat = np.array(test_dataset, dtype='int')\n",
    "print test_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498687,)\n"
     ]
    }
   ],
   "source": [
    "ans = net.predict(test_mat)\n",
    "print ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>act1_249281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act2_230855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>act1_240724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>act1_83552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>act2_1043301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    activity_id  outcome\n",
       "0   act1_249281        1\n",
       "1   act2_230855        1\n",
       "2   act1_240724        1\n",
       "3    act1_83552        1\n",
       "4  act2_1043301        1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame({'activity_id' : activity_id, 'outcome': ans})\n",
    "output_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "output_df.sort(['activity_id','outcome']).to_csv('output_2NN.out', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAH4CAYAAAAsMRvPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114623410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train', color='blue')\n",
    "plt.plot(stats['val_acc_history'], label='val', color='red')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-b31a8c78de82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mshow_net_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-b31a8c78de82>\u001b[0m in \u001b[0;36mshow_net_weights\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_net_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualize_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "  W1 = net.params['W1']\n",
    "  W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "  plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "  plt.gca().axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your hyperparameters\n",
    "\n",
    "**What's wrong?**. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n",
    "\n",
    "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. For every 1% above 52% on the Test set we will award you with one extra bonus point. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-12 11:22:28.938834 val_acc: 0.716620160595\tlr: 0.000122574676049\treg: 0.000257796641079\t0/100\n",
      "2016-08-12 11:22:39.934928 val_acc: 0.632343194896\tlr: 0.000224158476177\treg: 0.000557655839593\t1/100\n",
      "2016-08-12 11:22:50.251901 val_acc: 0.613925652863\tlr: 0.000139536206552\treg: 0.00018945940003\t2/100\n",
      "2016-08-12 11:23:02.027637 val_acc: 0.625695813316\tlr: 0.000682871192088\treg: 2.70580154327e-05\t3/100\n",
      "2016-08-12 11:23:12.083339 val_acc: 0.621824573979\tlr: 0.000455818449597\treg: 0.000766888962908\t4/100\n",
      "2016-08-12 11:23:22.053200 val_acc: 0.610489607955\tlr: 0.000331041148314\treg: 0.103990693092\t5/100\n",
      "2016-08-12 11:23:33.175227 val_acc: 0.614995150284\tlr: 0.000732548512665\treg: 9.24054008291\t6/100\n",
      "2016-08-12 11:23:46.972642 val_acc: 0.623909525069\tlr: 0.000990601701381\treg: 0.0605841418631\t7/100\n",
      "2016-08-12 11:24:00.459367 val_acc: 0.621227248291\tlr: 0.00097241506152\treg: 0.000256972431957\t8/100\n",
      "2016-08-12 11:24:13.233307 val_acc: 0.629535764164\tlr: 0.000626006291516\treg: 5.51949924912e-05\t9/100\n",
      "2016-08-12 11:24:27.339001 val_acc: 0.618237775445\tlr: 0.000756508731702\treg: 1.01650847341e-05\t10/100\n",
      "2016-08-12 11:24:39.370144 val_acc: 0.631819823817\tlr: 0.00012472432149\treg: 0.0188278192968\t11/100\n",
      "2016-08-12 11:24:53.146477 val_acc: 0.625752701477\tlr: 0.000293648989612\treg: 0.180120337567\t12/100\n",
      "2016-08-12 11:25:03.375673 val_acc: 0.630408997431\tlr: 0.000300267699586\treg: 3.00872002347e-05\t13/100\n",
      "2016-08-12 11:25:13.881646 val_acc: 0.619995619612\tlr: 0.000466586830712\treg: 0.000117850520794\t14/100\n",
      "2016-08-12 11:25:26.672048 val_acc: 0.618328796503\tlr: 0.000137417909563\treg: 0.41256937519\t15/100\n",
      "2016-08-12 11:25:39.614325 val_acc: 0.616741616818\tlr: 0.000408655904394\treg: 1.4465926638e-05\t16/100\n",
      "2016-08-12 11:25:54.758900 val_acc: 0.610754137903\tlr: 0.000867344845591\treg: 0.0346330997992\t17/100\n",
      "2016-08-12 11:26:08.701693 val_acc: 0.610088546422\tlr: 0.00021381414899\treg: 0.48482013355\t18/100\n",
      "2016-08-12 11:26:21.306118 val_acc: 0.63662402899\tlr: 0.000128205865173\treg: 0.025727330205\t19/100\n",
      "2016-08-12 11:26:33.626559 val_acc: 0.629202968424\tlr: 0.000172145596978\treg: 0.00107690260032\t20/100\n",
      "2016-08-12 11:26:44.442710 val_acc: 0.617612005677\tlr: 0.000731356457192\treg: 0.18472311217\t21/100\n",
      "2016-08-12 11:26:54.988951 val_acc: 0.607943862763\tlr: 0.00031106519343\treg: 2.6117201136e-05\t22/100\n",
      "2016-08-12 11:27:07.799512 val_acc: 0.617225166185\tlr: 0.000261523729472\treg: 0.000404128823741\t23/100\n",
      "2016-08-12 11:27:21.038158 val_acc: 0.617452718827\tlr: 0.000190100860718\treg: 3.91349924805\t24/100\n",
      "2016-08-12 11:27:34.807049 val_acc: 0.625394306064\tlr: 0.000206496793431\treg: 0.153406624131\t25/100\n",
      "2016-08-12 11:27:45.303577 val_acc: 0.624612093854\tlr: 0.00029206154603\treg: 0.0302035337724\t26/100\n",
      "2016-08-12 11:27:57.372319 val_acc: 0.62700708542\tlr: 0.000704501188147\treg: 0.000513460895598\t27/100\n",
      "2016-08-12 11:28:10.936857 val_acc: 0.615777362494\tlr: 0.000469198378771\treg: 3.00942246336\t28/100\n",
      "2016-08-12 11:28:21.398681 val_acc: 0.62037392588\tlr: 0.000377908624942\treg: 0.0016990918258\t29/100\n",
      "2016-08-12 11:28:31.579954 val_acc: 0.625169597829\tlr: 0.000418045290926\treg: 1.85294825627e-05\t30/100\n",
      "2016-08-12 11:28:43.187696 val_acc: 0.615453099978\tlr: 0.000872213430208\treg: 0.00329579965717\t31/100\n",
      "2016-08-12 11:28:56.062809 val_acc: 0.61815813202\tlr: 0.000312867804199\treg: 2.61466665319\t32/100\n",
      "2016-08-12 11:29:07.324573 val_acc: 0.624998933347\tlr: 0.000140685927101\treg: 0.137639553396\t33/100\n",
      "2016-08-12 11:29:17.358221 val_acc: 0.624185432649\tlr: 0.000165388855382\treg: 0.00267727353082\t34/100\n",
      "2016-08-12 11:29:27.532925 val_acc: 0.624171210609\tlr: 0.000325524101778\treg: 1.32616830204\t35/100\n",
      "2016-08-12 11:29:37.646722 val_acc: 0.625900610694\tlr: 0.000155337724843\treg: 0.000267722005073\t36/100\n",
      "2016-08-12 11:29:47.723629 val_acc: 0.625388617248\tlr: 0.000155983417595\treg: 0.000201902460466\t37/100\n",
      "2016-08-12 11:29:57.793765 val_acc: 0.615777362494\tlr: 0.000251309828842\treg: 0.0205849980049\t38/100\n",
      "2016-08-12 11:30:07.865952 val_acc: 0.628562976616\tlr: 0.000200224819812\treg: 0.933307451165\t39/100\n",
      "2016-08-12 11:30:17.993888 val_acc: 0.629598341141\tlr: 0.000169568597078\treg: 0.0015689543802\t40/100\n",
      "2016-08-12 11:30:28.129619 val_acc: 0.634129483143\tlr: 0.00054805222872\treg: 0.00289389287563\t41/100\n",
      "2016-08-12 11:30:38.286140 val_acc: 0.638973510028\tlr: 0.000167296312008\treg: 0.372537604492\t42/100\n",
      "2016-08-12 11:30:48.401227 val_acc: 0.627473568338\tlr: 0.000153356378875\treg: 0.0185337910506\t43/100\n",
      "2016-08-12 11:30:58.582628 val_acc: 0.606430637688\tlr: 0.000954462698284\treg: 0.020175194564\t44/100\n",
      "2016-08-12 11:31:08.702626 val_acc: 0.623770149075\tlr: 0.000722640251582\treg: 0.271086125809\t45/100\n",
      "2016-08-12 11:31:18.865274 val_acc: 0.611547727745\tlr: 0.00019370411712\treg: 0.00131849318955\t46/100\n",
      "2016-08-12 11:31:29.075934 val_acc: 0.605079543871\tlr: 0.000348591682893\treg: 0.000105389908127\t47/100\n",
      "2016-08-12 11:31:39.159023 val_acc: 0.618946033046\tlr: 0.000276529767856\treg: 1.17875252768\t48/100\n",
      "2016-08-12 11:31:53.343165 val_acc: 0.64134574633\tlr: 0.000539169552103\treg: 0.0335967484661\t49/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-054032c9a16b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             reg=reg, verbose=False)\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jojas/kaggle-redhat/sakares-sandbox/cs231n/classifiers/neural_net.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, learning_rate, learning_rate_decay, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m#########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;31m#########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_net.                                                            #\n",
    "#                                                                               #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
    "# write code to sweep through possible combinations of hyperparameters          #\n",
    "# automatically like we did on the previous exercises.                          #\n",
    "#################################################################################\n",
    "import datetime\n",
    "\n",
    "# input_size = 32 * 32 * 3\n",
    "# hidden_size = 100\n",
    "# num_classes = 10\n",
    "# max_count = 100\n",
    "\n",
    "input_size = 50\n",
    "hidden_size = 30\n",
    "num_classes = 2\n",
    "max_count = 100\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "best_val = 0\n",
    "best_reg = 0\n",
    "best_lr = 0\n",
    "\n",
    "for count in xrange(max_count):\n",
    "    reg = 10**np.random.uniform(-5,1)\n",
    "    lr = 10**np.random.uniform(-3,-4)    \n",
    "    net_cur = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=10000, batch_size=200,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "    val_acc = (net.predict(X_val) == y_val).mean()\n",
    "    if val_acc > best_val:\n",
    "        best_net = net_cur\n",
    "        best_val = val_acc\n",
    "        best_lr  = lr\n",
    "        best_reg = reg\n",
    "    print str(datetime.datetime.now())+' val_acc: '+str(val_acc)+'\\tlr: '+str(lr)+'\\treg: '+str(reg)+'\\t'+str(count)+'/'+str(max_count)\n",
    "\n",
    "print \"best_reg: \"+str(best_reg)\n",
    "print \"best_lr: \"+str(best_lr)\n",
    "\n",
    "#################################################################################\n",
    "#                               END OF YOUR CODE                                #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_net_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-92183bf1948b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize the weights of the best network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow_net_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_net_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.\n",
    "\n",
    "**We will give you extra bonus point for every 1% of accuracy above 52%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-758052de9fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Test accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print 'Test accuracy: ', test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
